from bs4 import BeautifulSoup
import pandas as pd
import re
import requests
from openai import OpenAI
import os
from collections import OrderedDict
import json
import streamlit as st
import io
import xlsxwriter
import openai
import time
from urllib.parse import urljoin
from urllib.parse import urlparse

# =========================
# OpenAI API Key (Cloud ì¤‘ì‹¬)
# =========================
api_key = st.secrets.get("OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY")
if not api_key:
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Cloud > Secretsì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=api_key)
openai.api_key = api_key

st.set_page_config(layout="wide", page_title="KEI ì°¸ê³ ë¬¸í—Œ ì˜¨ë¼ì¸ìë£Œ ê²€ì¦ë„êµ¬")


# =========================
# (ì„ íƒ) í…ìŠ¤íŠ¸ ìœ í‹¸
# =========================
def remove_duplicate_words(text):
    words = text.split()
    seen = OrderedDict()
    for word in words:
        if word not in seen:
            seen[word] = None
    return ' '.join(seen.keys())


def truncate_string(text, max_length=10000):
    return text[:max_length]


# =========================
# URL ìƒíƒœ ì²´í¬ (ì •ìƒ/ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€/ì •ìƒ(ë³´ì•ˆì£¼ì˜) + ë©”ëª¨)
# =========================
def check_url_status(url: str, timeout: int = 15) -> dict:
    if not isinstance(url, str) or not url.strip():
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "URL ì—†ìŒ"}

    url = url.strip()
    if not (url.startswith("http://") or url.startswith("https://")):
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "http/httpsë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ"}

    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        # ê¸°ë³¸: SSL ê²€ì¦ ON
        r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        status_code = r.status_code
        final_url = r.url

        if 200 <= status_code < 300:
            return {"URL_ìƒíƒœ": "ì •ìƒ", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": ""}
        else:
            return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": f"HTTP {status_code}"}

    except requests.exceptions.SSLError:
        # SSL ê²€ì¦ ì‹¤íŒ¨ì§€ë§Œ ì‹¤ì œ ì ‘ì†ì€ ë˜ëŠ”ì§€ verify=Falseë¡œ 1íšŒ ì¬ì‹œë„
        try:
            r2 = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
            status_code = r2.status_code
            final_url = r2.url

            if 200 <= status_code < 300:
                memo = "SSL ê²€ì¦ ì‹¤íŒ¨(ë³´ì•ˆì£¼ì˜): verify=Falseë¡œëŠ” ì ‘ì†ë¨"
                return {"URL_ìƒíƒœ": "ì •ìƒ(ë³´ì•ˆì£¼ì˜)", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": memo}
            else:
                memo = f"SSL ê²€ì¦ ì‹¤íŒ¨ + HTTP {status_code}(verify=False)"
                return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": memo}

        except Exception as e2:
            msg = f"{type(e2).__name__}: {str(e2)[:120]}"
            return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": f"SSL í•¸ë“œì…°ì´í¬ ì‹¤íŒ¨(verify=Falseë„ ì‹¤íŒ¨) - {msg}"}

    except requests.exceptions.Timeout:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Timeout"}
    except requests.exceptions.ConnectionError:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Connection error"}
    except requests.exceptions.InvalidURL:
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Invalid URL"}
    except requests.exceptions.MissingSchema:
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "URL ìŠ¤í‚¤ë§ˆ ëˆ„ë½(http/https)"}
    except Exception as e:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": f"ì˜ˆì™¸: {type(e).__name__}"}


# =========================
# crawling: URLì—ì„œ í˜ì´ì§€ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
# =========================
def crawling(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    doc_exts = ['.pdf', '.doc', '.docx', '.xls', '.xlsx',
                '.ppt', '.pptx', '.txt', '.csv', '.rtf']

    if any(ext in url for ext in doc_exts):
        try:
            response = requests.head(url, allow_redirects=True, timeout=5)
            if response.status_code == 200:
                return "íŒŒì¼ë‹¤ìš´ê°€ëŠ¥"
            else:
                return "íŒŒì¼ë‹¤ìš´ë¶ˆê°€"
        except requests.exceptions.RequestException:
            return "íŒŒì¼ë‹¤ìš´ë¶ˆê°€"

    try:
        response = requests.get(url, headers=headers, timeout=30, allow_redirects=True)
        response_text = response.text

        if "You need to enable JavaScript to run this app" in response.text:
            soup2 = BeautifulSoup(response.text, 'html.parser')
            text = soup2.get_text(separator=' ', strip=True)
            if len(text) < 200:
                return "í™•ì¸ë¶ˆê°€"

        match = re.search(r"location\.href\s*=\s*['\"]([^'\"]+)['\"]", response.text)
        if match:
            redirect_url = match.group(1)
            if "javascript:" not in redirect_url.lower():
                redirect_url = urljoin(url, redirect_url)
                response2 = requests.get(redirect_url, headers=headers, timeout=30, allow_redirects=True)
                response_text = response_text + response2.text

        response.encoding = 'utf-8'

        if response.status_code == 200:
            soup = BeautifulSoup(response_text, 'html.parser')

            meta = soup.find('meta', attrs={'charset': True})
            if meta and meta.get('charset') and meta['charset'].lower() != 'utf-8':
                response.encoding = meta['charset']
                soup = BeautifulSoup(response.text, 'html.parser')

            content = soup.get_text(strip=True)

            iframes = soup.find_all('iframe')
            iframe_contents = []

            for iframe in iframes:
                iframe_src = iframe.get('src')
                if iframe_src and iframe_src.strip():
                    iframe_url = urljoin(url, iframe_src)
                    parsed = urlparse(iframe_url)

                    if parsed.scheme not in ('http', 'https'):
                        continue
                    try:
                        iframe_response = requests.get(iframe_url, headers=headers, timeout=30, allow_redirects=True)
                        if iframe_response.status_code == 200:
                            iframe_soup = BeautifulSoup(iframe_response.content, 'html.parser')
                            iframe_content = iframe_soup.get_text(strip=True)
                            iframe_contents.append(iframe_content)
                    except Exception:
                        pass

            if iframe_contents:
                content += "\n\n" + "\n\n".join(iframe_contents)

            return content
        else:
            return "í™•ì¸ë¶ˆê°€"

    except Exception:
        return "í™•ì¸ë¶ˆê°€"


# =========================
# GPT ê¸°ë°˜ URL ë‚´ìš© íŒë³„
# =========================
max_len = 50000

def GPTclass(x, y):
    y = crawling(y)
    if isinstance(y, str) and len(y) > max_len:
        y = y[0:max_len]

    if y == "í™•ì¸ë¶ˆê°€":
        return "í™•ì¸ë¶ˆê°€"
    if y == "íŒŒì¼ë‹¤ìš´ê°€ëŠ¥":
        return "íŒŒì¼ë‹¤ìš´ê°€ëŠ¥(ë‚´ìš©í™•ì¸ë¶ˆê°€)"
    if y == "íŒŒì¼ë‹¤ìš´ë¶ˆê°€":
        return "íŒŒì¼ë‹¤ìš´ë¶ˆê°€"
    if "í™•ì¸í•„ìš”" in x:
        return "O(í˜•ì‹ì˜¤ë¥˜)"

    retries = 0
    max_retries = 5
    while retries < max_retries:
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "[[ì›¹ìë£Œ]]ì—ì„œ ë‚´ìš©ì´ ì£¼ì–´ì§„ [[ì •ë³´]] ê´€ë ¨ë‚´ìš©ì´ ëŒ€ëµì ìœ¼ë¡œ í¬í•¨ë˜ì–´ìˆìœ¼ë©´ X, ê´€ë ¨ë‚´ìš©ì´ ì•„ë‹ˆê±°ë‚˜, ë¹ˆí˜ì´ì§€ ë˜ëŠ” ì—†ëŠ” í˜ì´ì§€ë©´ O ì¶œë ¥"},
                    {"role": "user", "content": f"[[ì •ë³´]]: {x}, [[ì›¹ìë£Œ]] : {y}"}
                ]
            )
            return response.choices[0].message.content
        except openai.RateLimitError as e:
            time.sleep(getattr(e, "retry_after", 2) + 2)
            retries += 1
        except Exception:
            return "í™•ì¸ë¶ˆê°€"


# =========================
# (ì¶”ê°€) GPT URL ê²°ê³¼ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰½ê²Œ ë³€í™˜ + ì»¬ëŸ¼ëª… ë³€ê²½ì— ì‚¬ìš©
# =========================
def map_gpt_url_result(v):
    if v is None:
        return "í™•ì¸ë¶ˆê°€"
    if not isinstance(v, str):
        return "í™•ì¸ë¶ˆê°€"

    s = v.strip()

    if s == "í™•ì¸ë¶ˆê°€":
        return "í™•ì¸ë¶ˆê°€"
    if "íŒŒì¼ë‹¤ìš´ê°€ëŠ¥" in s:
        return "íŒŒì¼(ë‚´ìš©í™•ì¸ë¶ˆê°€)"
    if "íŒŒì¼ë‹¤ìš´ë¶ˆê°€" in s:
        return "í™•ì¸ë¶ˆê°€"

    if s == "X" or s.startswith("X"):
        return "ì¼ì¹˜(ìœ íš¨)"
    if s == "O" or s.startswith("O"):
        return "ë¶ˆì¼ì¹˜(ì˜¤ë¥˜)"

    return s


# =========================
# ì°¸ê³ ë¬¸í—Œ ë¶„ë¦¬
# =========================
def separator(entry):
    parts = [""] * 4

    if 'http' in entry:
        pattern_http = r',\s+(?=http)'
    else:
        pattern_http = r',\s+(?=ê²€ìƒ‰ì¼)'

    parts_http = re.split(pattern_http, entry)
    doc_info = parts_http[0]
    ref_info = parts_http[1] if len(parts_http) > 1 else ""

    if 'â€œ' in doc_info and 'â€' in doc_info:
        match = re.match(r'(.+?),\s*?â€œ(.*)â€', doc_info)
        if match:
            parts[0] = match.group(1).strip()
            parts[1] = f'â€œ{match.group(2)}â€'
        else:
            parts[0] = doc_info.strip()
    else:
        parts[0] = doc_info.strip()

    if 'http' in ref_info:
        pattern_ref = r',\s+(?=ê²€ìƒ‰ì¼)'
        parts_ref = re.split(pattern_ref, ref_info)
        parts[2] = parts_ref[0].strip()
        parts[3] = parts_ref[1].strip() if len(parts_ref) > 1 else ""
    else:
        parts[3] = ref_info.strip()

    return parts


# =========================
# GPT í˜•ì‹ ê²€ì¦ (í•­ìƒ dict ë°˜í™˜)
# =========================
def GPTcheck(doc):
    query = """
    ë‹¹ì‹ ì€ ê° ì¤„ë§ˆë‹¤ ì•„ë˜ í˜•ì‹ì— ë§ëŠ” ë¬¸í—Œ ì •ë³´ê°€ ì •í™•íˆ ì…ë ¥ë˜ì—ˆëŠ”ì§€ ê²€í† í•©ë‹ˆë‹¤. ê° ë¬¸í—Œ ì •ë³´ëŠ” ë‹¤ìŒì˜ 4ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:
    1. ì¶œì²˜
    2. ì œëª©: ë°˜ë“œì‹œ í°ë”°ì˜´í‘œ(" ")ë¡œ ê°ìŒˆ
    3. URL
    4. ê²€ìƒ‰ì¼: "ê²€ìƒ‰ì¼: yyyy.m.d." í˜•ì‹
    ì¶œë ¥: JSON {"ì˜¤ë¥˜ì—¬ë¶€":"X"} ë˜ëŠ” {"ì˜¤ë¥˜ì—¬ë¶€":"O(ì´ìœ )"}
    """

    retries = 0
    max_retries = 5

    while retries < max_retries:
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system", "content": f"{query}"},
                    {"role": "user", "content": f"ë¬¸ì„œ:{doc}"}
                ]
            )
            raw = response.choices[0].message.content
            result_dict = json.loads(raw)

            err = result_dict.get("ì˜¤ë¥˜ì—¬ë¶€")
            if not err:
                err = "O(ì˜¤ë¥˜ì—¬ë¶€ ëˆ„ë½)"

            return {"ì˜¤ë¥˜ì—¬ë¶€": err, "ì›ë¬¸": doc}

        except openai.RateLimitError as e:
            time.sleep(getattr(e, "retry_after", 2) + 2)
            retries += 1
        except Exception as e:
            return {"ì˜¤ë¥˜ì—¬ë¶€": f"O(GPTcheck ì‹¤íŒ¨:{type(e).__name__})", "ì›ë¬¸": doc}


# =========================
# ê·œì¹™ ê¸°ë°˜ í˜•ì‹ ì²´í¬
# =========================
def check_format(text):
    title_match = re.search(r'"[^"]*"', text)
    if not title_match:
        return False

    title_start = title_match.start()
    title_end = title_match.end()
    title = text[title_start:title_end].strip()

    author = text[:title_start].strip().rstrip(',')
    if not author:
        return False

    rest = text[title_end:].strip()

    temp_parts = [p.strip() for p in re.split(r',(?=(?:[^"]*"[^"]*")*[^"]*$)', rest)]

    parts = []
    i = 0
    while i < len(temp_parts):
        part = temp_parts[i]
        if part.startswith("http"):
            while i + 1 < len(temp_parts) and not temp_parts[i+1].startswith("ê²€ìƒ‰ì¼") and not re.search(r'\d{4}', temp_parts[i+1]):
                part += ',' + temp_parts[i+1]
                i += 1
        parts.append(part)
        i += 1

    if len(parts) < 2:
        return False

    all_parts = [author, title] + parts[-2:]
    return len(all_parts) == 4


# =========================
# entries -> DataFrame
# =========================
def process_entries(entries):
    articles = []
    for entry in entries:
        note = ""
        if not check_format(entry):
            note = "í™•ì¸í•„ìš”"

        check = separator(entry)
        check = ["í™•ì¸í•„ìš”" if item == 'NA' or item == '' else item for item in check]
        source = check[0]

        if re.search(r"\d{2,4}\.\d+\.\d+", source):
            if not re.search(r"\b\d{4}\.([1-9]|1[0-2])\.([1-9]|[12][0-9]|3[01])\b", source):
                note = "í™•ì¸í•„ìš”"

        title = check[1]
        url = check[2]

        search_date = check[3].replace("ê²€ìƒ‰ì¼: ", "")
        search_date = search_date.strip()
        if not re.search(r"\b\d{4}\.([1-9]|1[0-2])\.([1-9]|[12][0-9]|3[01])\b", search_date):
            search_date = "í™•ì¸í•„ìš”"

        url_result = check_url_status(url)

        articles.append({
            "URL_ìƒíƒœ": url_result["URL_ìƒíƒœ"],
            "URL_ë©”ëª¨": url_result["URL_ë©”ëª¨"],
            "URL_ìƒíƒœì½”ë“œ": url_result["URL_ìƒíƒœì½”ë“œ"],
            "URL_ìµœì¢…URL": url_result["URL_ìµœì¢…URL"],

            "source": source,
            "title": title,
            "URL": url,
            "search_date": search_date,
            "í˜•ì‹ì²´í¬_ì˜¤ë¥˜ì—¬ë¶€": note
        })

    df = pd.DataFrame(articles)

    preferred_order = [
        "URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL_ìƒíƒœì½”ë“œ", "URL_ìµœì¢…URL",
        "source", "title", "URL", "search_date", "í˜•ì‹ì²´í¬_ì˜¤ë¥˜ì—¬ë¶€"
    ]
    cols = [c for c in preferred_order if c in df.columns] + [c for c in df.columns if c not in preferred_order]
    return df[cols]


# =========================
# Streamlit UI
# =========================
def main():
    st.title("KEI ì°¸ê³ ë¬¸í—Œ ì˜¨ë¼ì¸ìë£Œ ê²€ì¦ë„êµ¬")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "text_data" not in st.session_state:
        st.session_state["text_data"] = ""
    if "processed_data" not in st.session_state:
        st.session_state["processed_data"] = None
    if "result_df" not in st.session_state:
        st.session_state["result_df"] = None  # âœ… ê²°ê³¼ DF ì €ì¥ (ìˆ˜ë™ ì…ë ¥ ìœ ì§€)

    uploaded_file = st.file_uploader(
        "ë³´ê³ ì„œ ì°¸ê³ ë¬¸í—Œ ì¤‘ ì˜¨ë¼ì¸ìë£Œì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼(txt)ë¥¼ ì—…ë¡œë“œ í•˜ê±°ë‚˜ ",
        type=["txt"]
    )
    text_data = st.text_area(
        "ë˜ëŠ” ì•„ë˜ì— ì˜¨ë¼ì¸ìë£Œì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
        st.session_state["text_data"],
        height=300
    )

    col_run, col_reset = st.columns([1, 1])
    with col_run:
        run_clicked = st.button("ğŸ‘‰ì—¬ê¸°ë¥¼ ëˆŒëŸ¬, ê²€ì¦ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
    with col_reset:
        reset_clicked = st.button("ğŸ”ƒ(ê²€ì¦ í›„)ìˆ˜ë™ ì…ë ¥/ê²°ê³¼ ì´ˆê¸°í™” ë²„íŠ¼")

    if reset_clicked:
        st.session_state["processed_data"] = None
        st.session_state["result_df"] = None
        st.success("ì´ˆê¸°í™” ì™„ë£Œ! ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        st.stop()

    # =========================
    # ê²€ì¦ ì‹¤í–‰
    # =========================
    if run_clicked:
        progress_bar = st.progress(0)
        status_text = st.empty()

        if not (uploaded_file or text_data.strip()):
            st.warning("í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()

        progress_bar.progress(5)
        status_text.text("1ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ë¡œë”© ì¤‘...")

        if uploaded_file:
            data = uploaded_file.read().decode("utf-8")
        else:
            data = text_data

        entries = data.strip().splitlines()

        progress_bar.progress(10)
        status_text.text("2ë‹¨ê³„: ê¸°ë³¸ í˜•ì‹ ë° URL ì²´í¬ ì¤‘...")

        result_df = process_entries(entries)

        status_text.text("3ë‹¨ê³„: GPT í˜•ì‹ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
        GPT_check_list = []
        n3 = len(entries)

        for idx, doc in enumerate(entries):
            GPT_check_list.append(GPTcheck(doc))
            progress = 15 + int(30 * (idx + 1) / max(n3, 1))
            progress_bar.progress(progress)
            status_text.text(f"3ë‹¨ê³„: GPT í˜•ì‹ê²€ì¦ ìˆ˜í–‰ ì¤‘... ({idx + 1}/{n3})")

        gpt_errors = []
        gpt_originals = []
        for r, doc in zip(GPT_check_list, entries):
            if isinstance(r, dict):
                gpt_errors.append(r.get("ì˜¤ë¥˜ì—¬ë¶€", "O(ì˜¤ë¥˜ì—¬ë¶€ ì—†ìŒ)"))
                gpt_originals.append(r.get("ì›ë¬¸", doc))
            else:
                gpt_errors.append("O(GPTcheck None)")
                gpt_originals.append(doc)

        result_df["GPT_í˜•ì‹ì²´í¬_ì˜¤ë¥˜ì—¬ë¶€"] = gpt_errors
        result_df["ì›ë¬¸"] = gpt_originals

        status_text.text("4ë‹¨ê³„: GPT ê¸°ë°˜ URL ë‚´ìš© ê²€ì¦ ì¤‘...")
        n4 = len(result_df)
        URL_check_results = []

        for i, (title_source, url) in enumerate(zip(result_df["title"] + " + " + result_df["source"], result_df["URL"])):
            URL_check_results.append(GPTclass(title_source, url))
            progress = 45 + int(50 * (i + 1) / max(n4, 1))
            progress_bar.progress(progress)
            status_text.text(f"4ë‹¨ê³„: URL í™•ì¸ ì¤‘... ({i + 1}/{n4})")

        # âœ… ì»¬ëŸ¼ëª… ë³€ê²½ + X/O -> í•œê¸€ ë¼ë²¨ ë³€í™˜
        result_df["URL_ë‚´ìš©ì¼ì¹˜ì—¬ë¶€(GPT)"] = [map_gpt_url_result(x) for x in URL_check_results]

        # ===== ìˆ˜ë™/ìµœì¢… ì»¬ëŸ¼ ì¤€ë¹„ =====
        result_df["ìˆ˜ë™_URL_ìƒíƒœ"] = ""
        result_df["ìˆ˜ë™_ë©”ëª¨"] = ""
        result_df["ìµœì¢…_URL_ìƒíƒœ"] = result_df["URL_ìƒíƒœ"]
        result_df["ìµœì¢…_URL_ë©”ëª¨"] = result_df["URL_ë©”ëª¨"]

        # ìµœì¢… ì»¬ëŸ¼ì„ ì•ìª½ìœ¼ë¡œ
        front_cols = ["ìµœì¢…_URL_ìƒíƒœ", "ìµœì¢…_URL_ë©”ëª¨", "URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL_ìƒíƒœì½”ë“œ", "URL_ìµœì¢…URL"]
        front_cols = [c for c in front_cols if c in result_df.columns]
        result_df = result_df[front_cols + [c for c in result_df.columns if c not in front_cols]]

        progress_bar.progress(95)
        status_text.text("5ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬ ë° ìˆ˜ë™ í™•ì¸ ì…ë ¥ ì¤€ë¹„ ì¤‘...")

        # âœ… ì„¸ì…˜ì— ì €ì¥ (ë¦¬ëŸ°ì—ë„ ìˆ˜ë™ ì…ë ¥ ìœ ì§€)
        st.session_state["result_df"] = result_df

        progress_bar.progress(100)
        status_text.text("âœ… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ì—ì„œ ìˆ˜ë™ í™•ì¸ í›„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")

    # =========================
    # ê²°ê³¼ í‘œì‹œ(ì„¸ì…˜ì— ì €ì¥ëœ DF ê¸°ë°˜)
    # =========================
    if st.session_state["result_df"] is not None:
        result_df = st.session_state["result_df"]

        # âœ… (ìš”ì²­) Expander í—¤ë” ë°°ê²½ìƒ‰ ìŠ¤íƒ€ì¼ ì ìš© (ë²„íŠ¼ì²˜ëŸ¼ ê°•ì¡°)
        st.markdown("""
        <style>
        div[data-testid="stExpander"] details summary {
            background: #ffb2d9;       /* ì—°í•œ í•‘í¬ */
            border: 1px solid #ff997f; /* ì§„í•œ í…Œë‘ë¦¬ */
            border-radius: 10px;
            padding: 10px 12px;
            font-weight: 700;
        }
        div[data-testid="stExpander"] details summary svg {
            margin-right: 8px;
        }
        </style>
        """, unsafe_allow_html=True)

        # ===== ìˆ˜ë™ í™•ì¸ UI (ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€ë§Œ) =====
        with st.expander("ğŸ” ë‹´ë‹¹ìì˜ ìˆ˜ë™ í™•ì¸(ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€)ì´ í•„ìš”í•©ë‹ˆë‹¤. ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”! ì•„ë˜ í‘œê°€ í™œì„±í™”ë˜ë©´, URL(í´ë¦­)ì— ì ‘ì†í•˜ì—¬ ìµœì¢… íŒì • ê²°ê³¼ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.ğŸ¤—", expanded=False):
            issue_mask = result_df["URL_ìƒíƒœ"].isin(["ì˜¤ë¥˜", "í™•ì¸ë¶ˆê°€"])
            issues_df = result_df.loc[issue_mask, [
                "URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL", "source", "title", "ìˆ˜ë™_URL_ìƒíƒœ", "ìˆ˜ë™_ë©”ëª¨"
            ]].copy()

            if len(issues_df) == 0:
                st.info("ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•œ(ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€) í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                edited = st.data_editor(
                    issues_df,
                    use_container_width=True,
                    hide_index=False,
                    column_config={
                        "URL": st.column_config.LinkColumn("URL(í´ë¦­)", display_text="ì—´ê¸°"),
                        "ìˆ˜ë™_URL_ìƒíƒœ": st.column_config.SelectboxColumn(
                            "ìˆ˜ë™_URL_ìƒíƒœ(ì„ íƒ)",
                            options=["", "ì •ìƒ", "ì •ìƒ(ë³´ì•ˆì£¼ì˜)", "ì˜¤ë¥˜", "í™•ì¸ë¶ˆê°€"],
                            help="ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸í•œ ê²°ê³¼ë¥¼ ì„ íƒí•˜ì„¸ìš”."
                        ),
                        "ìˆ˜ë™_ë©”ëª¨": st.column_config.TextColumn(
                            "ìˆ˜ë™_ë©”ëª¨",
                            help="ìˆ˜ë™ í™•ì¸ ê·¼ê±°/ì‚¬ìœ ë¥¼ ê°„ë‹¨íˆ ì ì–´ë‘ì„¸ìš”."
                        ),
                    },
                    disabled=["URL_ìƒíƒœ", "URL_ë©”ëª¨", "source", "title"],
                    key="manual_editor",
                )

                if st.button("âœ… ìˆ˜ë™ íŒì • ì ìš©"):
                    result_df.loc[edited.index, "ìˆ˜ë™_URL_ìƒíƒœ"] = edited["ìˆ˜ë™_URL_ìƒíƒœ"]
                    result_df.loc[edited.index, "ìˆ˜ë™_ë©”ëª¨"] = edited["ìˆ˜ë™_ë©”ëª¨"]

                    has_manual = result_df["ìˆ˜ë™_URL_ìƒíƒœ"].astype(str).str.strip().ne("")
                    result_df.loc[has_manual, "ìµœì¢…_URL_ìƒíƒœ"] = result_df.loc[has_manual, "ìˆ˜ë™_URL_ìƒíƒœ"]

                    has_manual_memo = result_df["ìˆ˜ë™_ë©”ëª¨"].astype(str).str.strip().ne("")
                    result_df.loc[has_manual_memo, "ìµœì¢…_URL_ë©”ëª¨"] = result_df.loc[has_manual_memo, "ìˆ˜ë™_ë©”ëª¨"]

                    st.session_state["result_df"] = result_df
                    st.success("ìˆ˜ë™ íŒì •ì„ ìµœì¢… ê°’ì— ë°˜ì˜í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ í‘œ/ì—‘ì…€ì— ì ìš©ë©ë‹ˆë‹¤.")

        # âœ… í™”ë©´ì—ì„œ ìµœì¢…_URL_ìƒíƒœ ìƒ‰ì¹ (ê¸°ì¡´ ìœ ì§€)
        def highlight_url_status(val):
            if val == "ì˜¤ë¥˜":
                return "background-color: #f8d7da"  # ì—°í•œ ë¹¨ê°•
            if val == "í™•ì¸ë¶ˆê°€":
                return "background-color: #fff3cd"  # ì—°í•œ ë…¸ë‘
            if val == "ì •ìƒ(ë³´ì•ˆì£¼ì˜)":
                return "background-color: #ffe5b4"  # ì—°í•œ ì£¼í™©
            return ""

        styled = result_df.style.applymap(highlight_url_status, subset=["ìµœì¢…_URL_ìƒíƒœ"])
        st.dataframe(styled, use_container_width=True)

        # âœ… ì—‘ì…€ ì €ì¥ + ì¡°ê±´ë¶€ì„œì‹(ìµœì¢…_URL_ìƒíƒœ ê¸°ì¤€)
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            result_df.to_excel(writer, index=False, sheet_name='Sheet1')
            workbook = writer.book
            worksheet = writer.sheets['Sheet1']

            if "ìµœì¢…_URL_ìƒíƒœ" in result_df.columns:
                status_col = result_df.columns.get_loc("ìµœì¢…_URL_ìƒíƒœ")

                fmt_red = workbook.add_format({'bg_color': '#F8D7DA'})
                fmt_yel = workbook.add_format({'bg_color': '#FFF3CD'})
                fmt_org = workbook.add_format({'bg_color': '#FFE5B4'})

                start_row = 1
                end_row = len(result_df)

                worksheet.conditional_format(start_row, status_col, end_row, status_col, {
                    'type': 'text',
                    'criteria': 'containing',
                    'value': 'ì˜¤ë¥˜',
                    'format': fmt_red
                })
                worksheet.conditional_format(start_row, status_col, end_row, status_col, {
                    'type': 'text',
                    'criteria': 'containing',
                    'value': 'í™•ì¸ë¶ˆê°€',
                    'format': fmt_yel
                })
                worksheet.conditional_format(start_row, status_col, end_row, status_col, {
                    'type': 'text',
                    'criteria': 'containing',
                    'value': 'ì •ìƒ(ë³´ì•ˆì£¼ì˜)',
                    'format': fmt_org
                })

        output.seek(0)
        st.session_state["processed_data"] = output.read()

        if st.session_state["processed_data"]:
            st.download_button(
                label="ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ",
                data=st.session_state["processed_data"],
                file_name="result.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )


if __name__ == "__main__":
    main()
