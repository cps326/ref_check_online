from bs4 import BeautifulSoup
import pandas as pd
import re
import requests
from openai import OpenAI
import os
from collections import OrderedDict
import json
import streamlit as st
import io
import xlsxwriter
import openai
import time
from urllib.parse import urljoin, urlparse

# =========================
# OpenAI API Key (Cloud ì¤‘ì‹¬)
# =========================
api_key = st.secrets.get("OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY")
if not api_key:
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Cloud > Secretsì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=api_key)
openai.api_key = api_key

st.set_page_config(layout="wide", page_title="KEI ì°¸ê³ ë¬¸í—Œ ì˜¨ë¼ì¸ìë£Œ ê²€ì¦ë„êµ¬")


# =========================
# (ì„ íƒ) í…ìŠ¤íŠ¸ ìœ í‹¸
# =========================
def remove_duplicate_words(text):
    words = text.split()
    seen = OrderedDict()
    for word in words:
        if word not in seen:
            seen[word] = None
    return " ".join(seen.keys())


def truncate_string(text, max_length=10000):
    return text[:max_length]


# =========================
# URL ìƒíƒœ ì²´í¬
# =========================
def check_url_status(url: str, timeout: int = 15) -> dict:
    if not isinstance(url, str) or not url.strip():
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "URL ì—†ìŒ"}

    url = url.strip()
    if not (url.startswith("http://") or url.startswith("https://")):
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "http/httpsë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ"}

    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        status_code = r.status_code
        final_url = r.url

        if 200 <= status_code < 300:
            return {"URL_ìƒíƒœ": "ì •ìƒ", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": ""}
        else:
            return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": f"HTTP {status_code}"}

    except requests.exceptions.SSLError:
        try:
            r2 = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
            status_code = r2.status_code
            final_url = r2.url

            if 200 <= status_code < 300:
                memo = "SSL ê²€ì¦ ì‹¤íŒ¨(ë³´ì•ˆì£¼ì˜): verify=Falseë¡œëŠ” ì ‘ì†ë¨"
                return {"URL_ìƒíƒœ": "ì •ìƒ(ë³´ì•ˆì£¼ì˜)", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": memo}
            else:
                memo = f"SSL ê²€ì¦ ì‹¤íŒ¨ + HTTP {status_code}(verify=False)"
                return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": memo}

        except Exception as e2:
            msg = f"{type(e2).__name__}: {str(e2)[:120]}"
            return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": f"SSL í•¸ë“œì…°ì´í¬ ì‹¤íŒ¨(verify=Falseë„ ì‹¤íŒ¨) - {msg}"}

    except requests.exceptions.Timeout:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Timeout"}
    except requests.exceptions.ConnectionError:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Connection error"}
    except requests.exceptions.InvalidURL:
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Invalid URL"}
    except requests.exceptions.MissingSchema:
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "URL ìŠ¤í‚¤ë§ˆ ëˆ„ë½(http/https)"}
    except Exception as e:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": f"ì˜ˆì™¸: {type(e).__name__}"}


# =========================
# crawling: URLì—ì„œ í˜ì´ì§€ í…ìŠ¤íŠ¸
# =========================
def crawling(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }
    doc_exts = [".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".txt", ".csv", ".rtf"]

    if not isinstance(url, str) or not url.strip():
        return "í™•ì¸ë¶ˆê°€"

    if any(ext in url for ext in doc_exts):
        try:
            response = requests.head(url, allow_redirects=True, timeout=5)
            return "íŒŒì¼ë‹¤ìš´ê°€ëŠ¥" if response.status_code == 200 else "íŒŒì¼ë‹¤ìš´ë¶ˆê°€"
        except requests.exceptions.RequestException:
            return "íŒŒì¼ë‹¤ìš´ë¶ˆê°€"

    try:
        response = requests.get(url, headers=headers, timeout=30, allow_redirects=True)
        response_text = response.text

        if "You need to enable JavaScript to run this app" in response_text:
            soup2 = BeautifulSoup(response_text, "html.parser")
            text = soup2.get_text(separator=" ", strip=True)
            if len(text) < 200:
                return "í™•ì¸ë¶ˆê°€"

        match = re.search(r"location\.href\s*=\s*['\"]([^'\"]+)['\"]", response_text)
        if match:
            redirect_url = match.group(1)
            if "javascript:" not in redirect_url.lower():
                redirect_url = urljoin(url, redirect_url)
                response2 = requests.get(redirect_url, headers=headers, timeout=30, allow_redirects=True)
                response_text = response_text + response2.text

        response.encoding = "utf-8"
        if response.status_code != 200:
            return "í™•ì¸ë¶ˆê°€"

        soup = BeautifulSoup(response_text, "html.parser")

        meta = soup.find("meta", attrs={"charset": True})
        if meta and meta.get("charset") and meta["charset"].lower() != "utf-8":
            response.encoding = meta["charset"]
            soup = BeautifulSoup(response.text, "html.parser")

        content = soup.get_text(strip=True)

        iframes = soup.find_all("iframe")
        iframe_contents = []
        for iframe in iframes:
            iframe_src = iframe.get("src")
            if not iframe_src or not iframe_src.strip():
                continue
            iframe_url = urljoin(url, iframe_src)
            parsed = urlparse(iframe_url)
            if parsed.scheme not in ("http", "https"):
                continue
            try:
                iframe_response = requests.get(iframe_url, headers=headers, timeout=30, allow_redirects=True)
                if iframe_response.status_code == 200:
                    iframe_soup = BeautifulSoup(iframe_response.content, "html.parser")
                    iframe_contents.append(iframe_soup.get_text(strip=True))
            except Exception:
                pass

        if iframe_contents:
            content += "\n\n" + "\n\n".join(iframe_contents)

        return content

    except Exception:
        return "í™•ì¸ë¶ˆê°€"


# =========================
# GPT URL íŒë³„ + ë§¤í•‘
# =========================
max_len = 50000

def GPTclass(x, y):
    y = crawling(y)
    if isinstance(y, str) and len(y) > max_len:
        y = y[:max_len]

    if y == "í™•ì¸ë¶ˆê°€":
        return "í™•ì¸ë¶ˆê°€"
    if y == "íŒŒì¼ë‹¤ìš´ê°€ëŠ¥":
        return "íŒŒì¼ë‹¤ìš´ê°€ëŠ¥(ë‚´ìš©í™•ì¸ë¶ˆê°€)"
    if y == "íŒŒì¼ë‹¤ìš´ë¶ˆê°€":
        return "íŒŒì¼ë‹¤ìš´ë¶ˆê°€"
    if isinstance(x, str) and "í™•ì¸í•„ìš”" in x:
        return "O(í˜•ì‹ì˜¤ë¥˜)"

    retries = 0
    while retries < 5:
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "[[ì›¹ìë£Œ]]ì—ì„œ ë‚´ìš©ì´ ì£¼ì–´ì§„ [[ì •ë³´]] ê´€ë ¨ë‚´ìš©ì´ ëŒ€ëµì ìœ¼ë¡œ í¬í•¨ë˜ì–´ìˆìœ¼ë©´ X, ê´€ë ¨ë‚´ìš©ì´ ì•„ë‹ˆê±°ë‚˜, ë¹ˆí˜ì´ì§€ ë˜ëŠ” ì—†ëŠ” í˜ì´ì§€ë©´ O ì¶œë ¥"},
                    {"role": "user", "content": f"[[ì •ë³´]]: {x}, [[ì›¹ìë£Œ]] : {y}"}
                ],
            )
            return response.choices[0].message.content
        except openai.RateLimitError as e:
            time.sleep(getattr(e, "retry_after", 2) + 2)
            retries += 1
        except Exception:
            return "í™•ì¸ë¶ˆê°€"


def map_gpt_url_result(v):
    if v is None or not isinstance(v, str):
        return "í™•ì¸ë¶ˆê°€"
    s = v.strip()

    if s == "í™•ì¸ë¶ˆê°€":
        return "í™•ì¸ë¶ˆê°€"
    if "íŒŒì¼ë‹¤ìš´ê°€ëŠ¥" in s:
        return "íŒŒì¼(ë‚´ìš©í™•ì¸ë¶ˆê°€)"
    if "íŒŒì¼ë‹¤ìš´ë¶ˆê°€" in s:
        return "í™•ì¸ë¶ˆê°€"

    if s == "X" or s.startswith("X"):
        return "ì¼ì¹˜(ìœ íš¨)"
    if s == "O" or s.startswith("O"):
        return "ë¶ˆì¼ì¹˜(ì˜¤ë¥˜)"
    return s


# =========================
# ì°¸ê³ ë¬¸í—Œ ë¶„ë¦¬ + ê·œì¹™ ì²´í¬
# =========================
def separator(entry):
    parts = [""] * 4
    if "http" in entry:
        pattern_http = r",\s+(?=http)"
    else:
        pattern_http = r",\s+(?=ê²€ìƒ‰ì¼)"

    parts_http = re.split(pattern_http, entry)
    doc_info = parts_http[0]
    ref_info = parts_http[1] if len(parts_http) > 1 else ""

    if "â€œ" in doc_info and "â€" in doc_info:
        match = re.match(r"(.+?),\s*?â€œ(.*)â€", doc_info)
        if match:
            parts[0] = match.group(1).strip()
            parts[1] = f"â€œ{match.group(2)}â€"
        else:
            parts[0] = doc_info.strip()
            parts[1] = ""
    else:
        parts[0] = doc_info.strip()
        parts[1] = ""

    if "http" in ref_info:
        pattern_ref = r",\s+(?=ê²€ìƒ‰ì¼)"
        parts_ref = re.split(pattern_ref, ref_info)
        parts[2] = parts_ref[0].strip()
        parts[3] = parts_ref[1].strip() if len(parts_ref) > 1 else ""
    else:
        parts[3] = ref_info.strip()

    return parts


def check_format(text):
    title_match = re.search(r'"[^"]*"', text)
    if not title_match:
        return False

    title_start = title_match.start()
    author = text[:title_start].strip().rstrip(",")
    if not author:
        return False

    rest = text[title_match.end():].strip()
    temp_parts = [p.strip() for p in re.split(r",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", rest)]

    parts = []
    i = 0
    while i < len(temp_parts):
        part = temp_parts[i]
        if part.startswith("http"):
            while i + 1 < len(temp_parts) and not temp_parts[i + 1].startswith("ê²€ìƒ‰ì¼") and not re.search(r"\d{4}", temp_parts[i + 1]):
                part += "," + temp_parts[i + 1]
                i += 1
        parts.append(part)
        i += 1

    if len(parts) < 2:
        return False

    return True


# =========================
# GPT í˜•ì‹ ê²€ì¦
# =========================
def GPTcheck(doc):
    query = """
    ë‹¹ì‹ ì€ ê° ì¤„ë§ˆë‹¤ ì•„ë˜ í˜•ì‹ì— ë§ëŠ” ë¬¸í—Œ ì •ë³´ê°€ ì •í™•íˆ ì…ë ¥ë˜ì—ˆëŠ”ì§€ ê²€í† í•©ë‹ˆë‹¤.
    1. ì¶œì²˜
    2. ì œëª©: ë°˜ë“œì‹œ í°ë”°ì˜´í‘œ(" ")ë¡œ ê°ìŒˆ
    3. URL
    4. ê²€ìƒ‰ì¼: "ê²€ìƒ‰ì¼: yyyy.m.d." í˜•ì‹
    ì¶œë ¥: JSON {"ì˜¤ë¥˜ì—¬ë¶€":"X"} ë˜ëŠ” {"ì˜¤ë¥˜ì—¬ë¶€":"O(ì´ìœ )"}
    """

    retries = 0
    while retries < 5:
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system", "content": query},
                    {"role": "user", "content": f"ë¬¸ì„œ:{doc}"},
                ],
            )
            raw = response.choices[0].message.content
            result_dict = json.loads(raw)
            err = result_dict.get("ì˜¤ë¥˜ì—¬ë¶€") or "O(ì˜¤ë¥˜ì—¬ë¶€ ëˆ„ë½)"
            return {"ì˜¤ë¥˜ì—¬ë¶€": err, "ì›ë¬¸": doc}
        except openai.RateLimitError as e:
            time.sleep(getattr(e, "retry_after", 2) + 2)
            retries += 1
        except Exception as e:
            return {"ì˜¤ë¥˜ì—¬ë¶€": f"O(GPTcheck ì‹¤íŒ¨:{type(e).__name__})", "ì›ë¬¸": doc}


# =========================
# entries -> DataFrame (âœ… ì»¬ëŸ¼ëª… í™•ì • ìƒì„±)
# =========================
def process_entries(entries):
    articles = []
    for entry in entries:
        rule_note = "" if check_format(entry) else "í™•ì¸í•„ìš”"

        s = separator(entry)
        s = ["í™•ì¸í•„ìš”" if item in ("NA", "", None) else item for item in s]

        ì‘ì„±ê¸°ê´€_ì‘ì„±ì = s[0]
        ì œëª© = s[1]
        URL_ë³´ê³ ì„œê¸°ì¤€ = s[2]

        search_date = s[3].replace("ê²€ìƒ‰ì¼: ", "").strip()
        if not re.search(r"\b\d{4}\.([1-9]|1[0-2])\.([1-9]|[12][0-9]|3[01])\b", search_date):
            search_date = "í™•ì¸í•„ìš”"

        url_result = check_url_status(URL_ë³´ê³ ì„œê¸°ì¤€)

        articles.append({
            "URL_ìƒíƒœ": url_result["URL_ìƒíƒœ"],
            "URL_ë©”ëª¨": url_result["URL_ë©”ëª¨"],
            "URL_ìƒíƒœì½”ë“œ": url_result["URL_ìƒíƒœì½”ë“œ"],
            "URL_ìˆ˜ì •ì•ˆ": url_result["URL_ìµœì¢…URL"],

            "ì‘ì„±ê¸°ê´€_ì‘ì„±ì": ì‘ì„±ê¸°ê´€_ì‘ì„±ì,
            "ì œëª©": ì œëª©,
            "URL_ë³´ê³ ì„œê¸°ì¤€": URL_ë³´ê³ ì„œê¸°ì¤€,

            "search_date": search_date,
            "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(ê·œì¹™ê¸°ë°˜)": rule_note,
        })

    df = pd.DataFrame(articles)

    # âœ… í˜¹ì‹œë¼ë„ ëˆ„ë½ë˜ë©´ ê°•ì œë¡œ ìƒì„±(ë°©ì–´)
    must_cols = [
        "URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL_ìƒíƒœì½”ë“œ", "URL_ìˆ˜ì •ì•ˆ",
        "ì‘ì„±ê¸°ê´€_ì‘ì„±ì", "ì œëª©", "URL_ë³´ê³ ì„œê¸°ì¤€",
        "search_date", "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(ê·œì¹™ê¸°ë°˜)"
    ]
    for c in must_cols:
        if c not in df.columns:
            df[c] = ""

    preferred_order = [
        "URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL_ìƒíƒœì½”ë“œ", "URL_ìˆ˜ì •ì•ˆ",
        "ì‘ì„±ê¸°ê´€_ì‘ì„±ì", "ì œëª©", "URL_ë³´ê³ ì„œê¸°ì¤€",
        "search_date", "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(ê·œì¹™ê¸°ë°˜)"
    ]
    return df[preferred_order]


# =========================
# (í•µì‹¬) ì»¬ëŸ¼ëª…/í•„ìˆ˜ì»¬ëŸ¼ ì •ë¦¬ í•¨ìˆ˜: run ì´í›„/ì„¸ì…˜ ë³µì›ì‹œì—ë„ ë³´ì •
# =========================
def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or len(df) == 0:
        return df

    # í˜¹ì‹œ ê³¼ê±° ì»¬ëŸ¼ëª…ì´ ì„ì—¬ìˆì„ ë•Œ ëŒ€ë¹„(ë¦¬ë„¤ì„)
    rename_map = {
        "source": "ì‘ì„±ê¸°ê´€_ì‘ì„±ì",
        "title": "ì œëª©",
        "URL": "URL_ë³´ê³ ì„œê¸°ì¤€",
        "URL_ìµœì¢…URL": "URL_ìˆ˜ì •ì•ˆ",
        "í˜•ì‹ì²´í¬_ì˜¤ë¥˜ì—¬ë¶€": "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(ê·œì¹™ê¸°ë°˜)",
        "GPT_í˜•ì‹ì²´í¬_ì˜¤ë¥˜ì—¬ë¶€": "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(GPTê¸°ë°˜)",
        "GPT_URL_ìœ íš¨ì •ë³´_ì˜¤ë¥˜ì—¬ë¶€": "URL_ë‚´ìš©ì¼ì¹˜ì—¬ë¶€(GPT)",
        "ìˆ˜ë™_URL_ìƒíƒœ": "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼",
        "ìˆ˜ë™_ë©”ëª¨": "ìˆ˜ë™ê²€ì¦_ë©”ëª¨",
    }
    for old, new in rename_map.items():
        if old in df.columns and new not in df.columns:
            df = df.rename(columns={old: new})

    must_cols = [
        "URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL_ìƒíƒœì½”ë“œ", "URL_ìˆ˜ì •ì•ˆ",
        "ì‘ì„±ê¸°ê´€_ì‘ì„±ì", "ì œëª©", "URL_ë³´ê³ ì„œê¸°ì¤€",
        "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼", "ìˆ˜ë™ê²€ì¦_ë©”ëª¨",
        "ìµœì¢…_URL_ìƒíƒœ", "ìµœì¢…_URL_ë©”ëª¨",
    ]
    for c in must_cols:
        if c not in df.columns:
            df[c] = ""

    # ìµœì¢…ì»¬ëŸ¼ ê¸°ë³¸ê°’
    if "ìµœì¢…_URL_ìƒíƒœ" in df.columns and df["ìµœì¢…_URL_ìƒíƒœ"].astype(str).str.strip().eq("").all():
        df["ìµœì¢…_URL_ìƒíƒœ"] = df.get("URL_ìƒíƒœ", "")
    if "ìµœì¢…_URL_ë©”ëª¨" in df.columns and df["ìµœì¢…_URL_ë©”ëª¨"].astype(str).str.strip().eq("").all():
        df["ìµœì¢…_URL_ë©”ëª¨"] = df.get("URL_ë©”ëª¨", "")

    return df


# =========================
# Streamlit UI
# =========================
def main():
    st.title("KEI ì°¸ê³ ë¬¸í—Œ ì˜¨ë¼ì¸ìë£Œ ê²€ì¦ë„êµ¬")

    if "processed_data" not in st.session_state:
        st.session_state["processed_data"] = None
    if "result_df" not in st.session_state:
        st.session_state["result_df"] = None

    uploaded_file = st.file_uploader(
        "ë³´ê³ ì„œ ì°¸ê³ ë¬¸í—Œ ì¤‘ ì˜¨ë¼ì¸ìë£Œì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼(txt)ë¥¼ ì—…ë¡œë“œ í•˜ê±°ë‚˜ ",
        type=["txt"],
    )
    text_data = st.text_area(
        "ë˜ëŠ” ì•„ë˜ì— ì˜¨ë¼ì¸ìë£Œì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
        "",
        height=300,
    )

    col_run, col_reset = st.columns([1, 1])
    with col_run:
        run_clicked = st.button("ğŸ‘‰ì—¬ê¸°ë¥¼ ëˆŒëŸ¬, ê²€ì¦ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
    with col_reset:
        reset_clicked = st.button("ğŸ”ƒ(ê²€ì¦ í›„)ìˆ˜ë™ ì…ë ¥/ê²°ê³¼ ì´ˆê¸°í™” ë²„íŠ¼")

    if reset_clicked:
        st.session_state["processed_data"] = None
        st.session_state["result_df"] = None
        st.success("ì´ˆê¸°í™” ì™„ë£Œ! ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        st.stop()

    if run_clicked:
        progress_bar = st.progress(0)
        status_text = st.empty()

        if not (uploaded_file or text_data.strip()):
            st.warning("í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()

        progress_bar.progress(5)
        status_text.text("1ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ë¡œë”© ì¤‘...")

        if uploaded_file:
            data = uploaded_file.read().decode("utf-8")
        else:
            data = text_data

        entries = data.strip().splitlines()

        progress_bar.progress(10)
        status_text.text("2ë‹¨ê³„: ê¸°ë³¸ í˜•ì‹ ë° URL ì²´í¬ ì¤‘...")

        result_df = process_entries(entries)

        status_text.text("3ë‹¨ê³„: GPT í˜•ì‹ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
        GPT_check_list = []
        n3 = len(entries)
        for idx, doc in enumerate(entries):
            GPT_check_list.append(GPTcheck(doc))
            progress = 15 + int(30 * (idx + 1) / max(n3, 1))
            progress_bar.progress(progress)
            status_text.text(f"3ë‹¨ê³„: GPT í˜•ì‹ê²€ì¦ ìˆ˜í–‰ ì¤‘... ({idx + 1}/{n3})")

        gpt_errors = []
        gpt_originals = []
        for r, doc in zip(GPT_check_list, entries):
            if isinstance(r, dict):
                gpt_errors.append(r.get("ì˜¤ë¥˜ì—¬ë¶€", "O(ì˜¤ë¥˜ì—¬ë¶€ ì—†ìŒ)"))
                gpt_originals.append(r.get("ì›ë¬¸", doc))
            else:
                gpt_errors.append("O(GPTcheck None)")
                gpt_originals.append(doc)

        result_df["ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(GPTê¸°ë°˜)"] = gpt_errors
        result_df["ì›ë¬¸"] = gpt_originals

        status_text.text("4ë‹¨ê³„: GPT ê¸°ë°˜ URL ë‚´ìš© ê²€ì¦ ì¤‘...")
        n4 = len(result_df)
        URL_check_results = []
        for i, (title_source, url) in enumerate(
            zip(
                result_df["ì œëª©"].astype(str) + " + " + result_df["ì‘ì„±ê¸°ê´€_ì‘ì„±ì"].astype(str),
                result_df["URL_ë³´ê³ ì„œê¸°ì¤€"].astype(str),
            )
        ):
            URL_check_results.append(GPTclass(title_source, url))
            progress = 45 + int(50 * (i + 1) / max(n4, 1))
            progress_bar.progress(progress)
            status_text.text(f"4ë‹¨ê³„: URL í™•ì¸ ì¤‘... ({i + 1}/{n4})")

        result_df["URL_ë‚´ìš©ì¼ì¹˜ì—¬ë¶€(GPT)"] = [map_gpt_url_result(x) for x in URL_check_results]

        # ìˆ˜ë™/ìµœì¢… ì»¬ëŸ¼ ìƒì„±
        result_df["URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"] = ""
        result_df["ìˆ˜ë™ê²€ì¦_ë©”ëª¨"] = ""
        result_df["ìµœì¢…_URL_ìƒíƒœ"] = result_df["URL_ìƒíƒœ"]
        result_df["ìµœì¢…_URL_ë©”ëª¨"] = result_df["URL_ë©”ëª¨"]

        # ì»¬ëŸ¼ ë³´ì •(í˜¹ì‹œë¼ë„ ê¼¬ì„ ë°©ì§€)
        result_df = ensure_required_columns(result_df)

        # ë³´ê¸° ì¢‹ê²Œ ì•ì—´ ë°°ì¹˜
        front_cols = ["ìµœì¢…_URL_ìƒíƒœ", "ìµœì¢…_URL_ë©”ëª¨", "URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL_ìƒíƒœì½”ë“œ", "URL_ìˆ˜ì •ì•ˆ"]
        front_cols = [c for c in front_cols if c in result_df.columns]
        result_df = result_df[front_cols + [c for c in result_df.columns if c not in front_cols]]

        st.session_state["result_df"] = result_df

        progress_bar.progress(100)
        status_text.text("âœ… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ì—ì„œ ìˆ˜ë™ í™•ì¸ í›„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")

    # =========================
    # ê²°ê³¼ í‘œì‹œ(ì„¸ì…˜ ê¸°ë°˜)
    # =========================
    if st.session_state["result_df"] is not None:
        result_df = ensure_required_columns(st.session_state["result_df"])

        st.markdown(
            """
            <style>
            div[data-testid="stExpander"] details summary {
                background: #e8f0fe;
                border: 1px solid #8ab4f8;
                border-radius: 10px;
                padding: 10px 12px;
                font-weight: 700;
            }
            </style>
            """,
            unsafe_allow_html=True,
        )

        with st.expander(
            "ğŸ” ë‹´ë‹¹ìì˜ ìˆ˜ë™ í™•ì¸(ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€)ì´ í•„ìš”í•©ë‹ˆë‹¤. ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”! ì•„ë˜ í‘œê°€ í™œì„±í™”ë˜ë©´, URL(í´ë¦­)ì— ì ‘ì†í•˜ì—¬ ìµœì¢… íŒì • ê²°ê³¼ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.ğŸ¤—",
            expanded=False,
        ):
            issue_mask = result_df["URL_ìƒíƒœ"].isin(["ì˜¤ë¥˜", "í™•ì¸ë¶ˆê°€"])

            want_cols = ["URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL_ë³´ê³ ì„œê¸°ì¤€", "ì‘ì„±ê¸°ê´€_ì‘ì„±ì", "ì œëª©", "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼", "ìˆ˜ë™ê²€ì¦_ë©”ëª¨"]
            exist_cols = [c for c in want_cols if c in result_df.columns]  # âœ… ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ(KeyError ë°©ì§€)

            issues_df = result_df.loc[issue_mask, exist_cols].copy()

            if len(issues_df) == 0:
                st.info("ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•œ(ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€) í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                edited = st.data_editor(
                    issues_df,
                    use_container_width=True,
                    column_config={
                        "URL_ë³´ê³ ì„œê¸°ì¤€": st.column_config.LinkColumn("URL(í´ë¦­)", display_text="ì—´ê¸°"),
                        "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼": st.column_config.SelectboxColumn(
                            "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼(ì„ íƒ)",
                            options=["", "ì •ìƒ", "ì •ìƒ(ë³´ì•ˆì£¼ì˜)", "ì˜¤ë¥˜", "í™•ì¸ë¶ˆê°€"],
                        ),
                        "ìˆ˜ë™ê²€ì¦_ë©”ëª¨": st.column_config.TextColumn("ìˆ˜ë™ê²€ì¦_ë©”ëª¨"),
                    },
                    disabled=[c for c in ["URL_ìƒíƒœ", "URL_ë©”ëª¨", "ì‘ì„±ê¸°ê´€_ì‘ì„±ì", "ì œëª©"] if c in issues_df.columns],
                    key="manual_editor",
                )

                if st.button("âœ… ìˆ˜ë™ íŒì • ì ìš©"):
                    if "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼" in edited.columns:
                        result_df.loc[edited.index, "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"] = edited["URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"]
                    if "ìˆ˜ë™ê²€ì¦_ë©”ëª¨" in edited.columns:
                        result_df.loc[edited.index, "ìˆ˜ë™ê²€ì¦_ë©”ëª¨"] = edited["ìˆ˜ë™ê²€ì¦_ë©”ëª¨"]

                    has_manual = result_df["URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"].astype(str).str.strip().ne("")
                    result_df.loc[has_manual, "ìµœì¢…_URL_ìƒíƒœ"] = result_df.loc[has_manual, "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"]

                    has_manual_memo = result_df["ìˆ˜ë™ê²€ì¦_ë©”ëª¨"].astype(str).str.strip().ne("")
                    result_df.loc[has_manual_memo, "ìµœì¢…_URL_ë©”ëª¨"] = result_df.loc[has_manual_memo, "ìˆ˜ë™ê²€ì¦_ë©”ëª¨"]

                    st.session_state["result_df"] = result_df
                    st.success("ìˆ˜ë™ íŒì •ì„ ìµœì¢… ê°’ì— ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.")

        # í™”ë©´ í‘œì‹œ
        def highlight_url_status(val):
            if val == "ì˜¤ë¥˜":
                return "background-color: #f8d7da"
            if val == "í™•ì¸ë¶ˆê°€":
                return "background-color: #fff3cd"
            if val == "ì •ìƒ(ë³´ì•ˆì£¼ì˜)":
                return "background-color: #ffe5b4"
            return ""

        styled = result_df.style.applymap(highlight_url_status, subset=["ìµœì¢…_URL_ìƒíƒœ"])
        st.dataframe(styled, use_container_width=True)

        # ì—‘ì…€ ì €ì¥
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            result_df.to_excel(writer, index=False, sheet_name="Sheet1")
        output.seek(0)
        st.session_state["processed_data"] = output.read()

        st.download_button(
            label="ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ",
            data=st.session_state["processed_data"],
            file_name="result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )


if __name__ == "__main__":
    main()
